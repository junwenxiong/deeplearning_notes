# 4.梯度爆炸与梯度消失

举个例子来描述一下，对于下图所示的神经网络，<font color="red">梯度消失问题发生是，接近于输出层的hidden layer3的权值更新相对正常，但前面的hidden layer1的权值更新会变得很慢，导致前面的层次权值几乎不变，仍接近于初始化的权值。</font>这就导致hidden layer1相当于只是一个映射层，对所有的输入做了一个同一映射。

<br>

![](_v_images/20200307212028699_31993.png =703x)

我们接下来讨论下这个问题为何会产生。以下图的反向传播为例（假设每一层只有一个神经元且对于每一层$y_i= \sigma(z_i)=\sigma(w_ix_i + b_i)$，其中$\sigma$为sigmoid函数）

![](_v_images/20200307212444712_9601.png =590x)

可以推导出

![](_v_images/20200307213239867_17972.png =792x)

而sigmoid的导数$\sigma^{'}(x)$如下图

![](_v_images/20200307213350221_16929.png =491x)

可见，$\sigma^{'}(x)$的最大值为$\frac{1}{4}$，而我们初始化的网络权值$|w|$通常小于1，

因此，$|\sigma^{'}(x)w| \leq \frac{1}{4}$，因此对于链式求导，网络层数越多，求导结果$\frac{\partial C}{\partial b_1}$越小，

因而导致梯度消失的情况出现。

那么，对于梯度爆炸问题的原因就显而易见了，即$|\sigma^{'}(x)w|\geq1$.


* 解决方案：
    * 可以考虑使用ReLU函数
    
    * LSTM网络    

    * batchnorm
    
    * 残差结构



**参考：**

    https://zhuanlan.zhihu.com/p/25631496
    https://blog.csdn.net/qq_25737169/article/details/78847691
